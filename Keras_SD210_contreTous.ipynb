{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras-SD210-contreTous.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fANF7EcThjkT",
        "colab_type": "code",
        "outputId": "6f9ef3b5-220b-41be-c0d2-bcccdf6ae079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#%%\n",
        "#%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "#import keras\n",
        "print(\"Using tensorflow version \" + str(tf.__version__))\n",
        "#print(\"Using keras version \" + str(keras.__version__))\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using tensorflow version 2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpXgFgD5Y2yW",
        "colab_type": "code",
        "outputId": "3f20bf80-ef5a-4c98-8f45-19a0f321982e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#%%\n",
        "# Load training data\n",
        "\n",
        "nrows_train = 1068504\n",
        "nrows_test = 0\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "\n",
        "xtrain = np.loadtxt(root_dir + 'xtrain_challenge.csv', delimiter=',', skiprows = 1, max_rows = nrows_train + nrows_test)\n",
        "ytrain = np.loadtxt(root_dir + 'ytrain_challenge.csv', delimiter=',', skiprows = 1, max_rows = nrows_train + nrows_test)\n",
        "ytrain = np.array(ytrain).reshape(nrows_train + nrows_test)\n",
        "# Check the number of observations and properties\n",
        "#print(xtrain[:3,])\n",
        "#print(ytrain[:10])\n",
        "#print(xtrain.shape)\n",
        "#print(ytrain.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the cell below, we perform data augmentation techniques.\n",
        "\n",
        "Our algorithm (decision trees) doesn't require scaled data, and allow their range to be heterogeneous. However, we can still mirror the data: since each sample is composed of two faces, and a certain range of features is separable with respect to one of the two faces, we can simply exchange them. This increases the size of the training set by a 2 factor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA2DgV5BY9Hm",
        "colab_type": "code",
        "outputId": "c97f3a6b-8ff3-460e-cdf0-d9900fc5776d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#%%\n",
        "# Pre-processing: we just remove the 13*2 first features, concerning only one of the two faces\n",
        "#xtrain_preprocessed = xtrain[:, 26:]\n",
        "xtrain = xtrain.astype('float32')\n",
        "xtest = np.loadtxt(root_dir + 'xtest_challenge.csv', delimiter=',', skiprows = 1).astype('float32')\n",
        "\n",
        "# We change the columns\n",
        "#l2_diff_train = np.sqrt((xtrain[:, 0:13] - xtrain[:, 13:26])**2)\n",
        "#l2_diff_test = np.sqrt((xtest[:, 0:13] - xtest[:, 13:26])**2)\n",
        "#xtrain = np.hstack((l2_diff_train, xtrain[:, 26:]))\n",
        "#xtest = np.hstack((l2_diff_test, xtest[:, 26:]))\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(xtrain)\n",
        "\n",
        "x_train_permuter = np.copy(xtrain)\n",
        "x_train_permuter[:, :13] = xtrain[:, 13:26]\n",
        "x_train_permuter[:, 13:26] = xtrain[:, :13]\n",
        "\n",
        "new_x_train = np.concatenate((xtrain, x_train_permuter), axis=0)\n",
        "new_y_train = np.concatenate((ytrain, ytrain), axis=0)\n",
        "\n",
        "#xtrain_preprocessed=scaler.transform(xtrain)\n",
        "xtrain_preprocessed = xtrain\n",
        "#xtest_preprocessed = scaler.transform(xtest)\n",
        "xtest_preprocessed = xtest\n",
        "#xtrain_preprocessed, xval_preprocessed, ytrain, yval = train_test_split(xtrain_preprocessed, ytrain, test_size=0.0) #KAMIKAZE\n",
        "print(xtrain_preprocessed.shape)\n",
        "print(ytrain.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1068504, 37)\n",
            "(1068504,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wDMZ8y57il9B"
      },
      "source": [
        "In the code cell below we apply cross-validation on the XGBClassifier model. We boost the learning of many decision trees which are considered as weak leaners.\n",
        "The most important parameters are :\n",
        "- learning_rate: similar to other gradient descent methods, the classifier perform more or less a gradient correction. A small learning rate will lead to very slow convergence, a big learning rate can make us miss the sweet spot.\n",
        "- n_estimators: to be considered as a trade-off with the learning rate. It is the number of rounds performed by the classifier during learning. A too great number will lead to over-fitting.\n",
        "- max_depth: the maximum depth related to the decision trees. The less it is, the more the decision trees are basic (hence having higher bias, which is not a problem for boosting)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY0OPTMiOC-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "874f8edc-343c-4084-81e2-5c81c925ddc8"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  boost = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,max_depth=6\n",
        "                        gamma=0, learning_rate=0.25, max_delta_step=0,\n",
        "                        min_child_weight=1, missing=None, n_estimators=700, nthread=-1,\n",
        "                        objective='binary:logistic', sampling_method='gradient_based',\n",
        "                        reg_alpha=0, reg_lambda=1, tree_method='gpu_hist',\n",
        "                        scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
        "  \n",
        "  #p_grid_boost = {'max_depth': [2,3,4,5,6,7,8,9,10], 'n_estimators':[500,600,700,800,900,1000]}   \n",
        "\n",
        "  #grid_boost = GridSearchCV(estimator=boost, param_grid=p_grid_boost, scoring=\"accuracy\", cv=5)\n",
        "  \n",
        "  #grid_boost.fit(xtrain_preprocessed, ytrain)\n",
        "  boost.fit(new_x_train, new_y_train)\n",
        "\n",
        "  #print(\"Best Score: {}\".format(grid_boost.best_score_))\n",
        "  #print(\"Best params: {}\".format(grid_boost.best_params_))\n",
        "  #p_boost = boost.predict(xval_preprocessed)\n",
        "  #print(p_boost.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Score: 0.9990847015822186\n",
            "Best params: {'max_depth': 6, 'n_estimators': 700}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maMpQLxXPaI2",
        "colab_type": "code",
        "outputId": "3817ecac-98d5-4543-ab28-bffa3123b465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#boost = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
        "                        gamma=0, learning_rate=0.25, max_delta_step=0, max_depth=6,\n",
        "                        min_child_weight=1, missing=None, n_estimators=700, nthread=-1,\n",
        "                        objective='binary:logistic', sampling_method='gradient_based',\n",
        "                        reg_alpha=0, reg_lambda=1, tree_method='gpu_hist',\n",
        "                        scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
        "#boost.fit(new_x_train, new_y_train)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.25, max_delta_step=0, max_depth=6,\n",
              "              min_child_weight=1, missing=None, n_estimators=700, n_jobs=1,\n",
              "              nthread=-1, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, sampling_method='gradient_based',\n",
              "              scale_pos_weight=1, seed=0, silent=True, subsample=1,\n",
              "              tree_method='gpu_hist', verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_-949YbQWQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = boost.predict(xtest_preprocessed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIxo19DJRm50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savetxt(root_dir + 'ytest_challenge_student_cv3.csv', output, fmt = '%1.0d', delimiter=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3lJPEpvZH4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout\n",
        "\n",
        "# START CODE HERE\n",
        "input_shape = 24\n",
        "output_shape = 2\n",
        "\n",
        "new_model = Sequential([\n",
        "    Dense(24, activation='relu', input_dim=(input_shape)),\n",
        "    Dense(10, activation='relu'),\n",
        "    Dense(6, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Bw1VvJxa-cY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The optimizers module provides a number of optimization algorithms for updating\n",
        "# a netwok parameters accoridng to the computed error gradints\n",
        "from keras import optimizers\n",
        "\n",
        "# START CODE HERE\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.08)\n",
        "\n",
        "new_model.compile(optimizer=sgd,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# END CODE HERE\n",
        "# We can now have a look at the defined model topology\n",
        "new_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvkN0rmKbB34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is where the actual training-testing happens\n",
        "# Number of epochs we want to train\n",
        "n_epochs = 2\n",
        "\n",
        "# START CODE HERE\n",
        "history = new_model.fit(xtrain_preprocessed, ytrain, batch_size=32, epochs=n_epochs, verbose=1, validation_data=(xval_preprocessed, yval))\n",
        "# END CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PZDmBoOkJTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = new_model.predict_classes(xtest_preprocessed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gfv4sgimx_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(pred.shape)\n",
        "output = pred #ok\n",
        "np.savetxt(root_dir + 'ytest_challenge_student.csv', output, fmt = '%1.0d', delimiter=',')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}