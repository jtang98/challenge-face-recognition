{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fANF7EcThjkT",
    "outputId": "6f9ef3b5-220b-41be-c0d2-bcccdf6ae079"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "GpXgFgD5Y2yW",
    "outputId": "3f20bf80-ef5a-4c98-8f45-19a0f321982e"
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "\n",
    "nrows_train = 1068504\n",
    "nrows_test = 0\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "root_dir = \"/content/gdrive/My Drive/\"\n",
    "\n",
    "xtrain = np.loadtxt(root_dir + 'xtrain_challenge.csv', delimiter=',', skiprows = 1, max_rows = nrows_train + nrows_test)\n",
    "ytrain = np.loadtxt(root_dir + 'ytrain_challenge.csv', delimiter=',', skiprows = 1, max_rows = nrows_train + nrows_test)\n",
    "ytrain = np.array(ytrain).reshape(nrows_train + nrows_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we perform data augmentation techniques.\n",
    "\n",
    "Our algorithm (decision trees) doesn't require scaled data, and allow their range to be heterogeneous. However, we can still mirror the data: since each sample is composed of two faces, and a certain range of features is separable with respect to one of the two faces, we can simply exchange them. This increases the size of the training set by a 2 factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HA2DgV5BY9Hm",
    "outputId": "c97f3a6b-8ff3-460e-cdf0-d9900fc5776d"
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# Pre-processing: we just remove the 13*2 first features, concerning only one of the two faces\n",
    "#xtrain_preprocessed = xtrain[:, 26:]\n",
    "xtrain = xtrain.astype('float32')\n",
    "xtest = np.loadtxt(root_dir + 'xtest_challenge.csv', delimiter=',', skiprows = 1).astype('float32')\n",
    "\n",
    "# We change the columns\n",
    "#l2_diff_train = np.sqrt((xtrain[:, 0:13] - xtrain[:, 13:26])**2)\n",
    "#l2_diff_test = np.sqrt((xtest[:, 0:13] - xtest[:, 13:26])**2)\n",
    "#xtrain = np.hstack((l2_diff_train, xtrain[:, 26:]))\n",
    "#xtest = np.hstack((l2_diff_test, xtest[:, 26:]))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(xtrain)\n",
    "\n",
    "x_train_permuter = np.copy(xtrain)\n",
    "x_train_permuter[:, :13] = xtrain[:, 13:26]\n",
    "x_train_permuter[:, 13:26] = xtrain[:, :13]\n",
    "\n",
    "new_x_train = np.concatenate((xtrain, x_train_permuter), axis=0)\n",
    "new_y_train = np.concatenate((ytrain, ytrain), axis=0)\n",
    "\n",
    "#xtrain_preprocessed=scaler.transform(xtrain)\n",
    "xtrain_preprocessed = xtrain\n",
    "#xtest_preprocessed = scaler.transform(xtest)\n",
    "xtest_preprocessed = xtest\n",
    "#xtrain_preprocessed, xval_preprocessed, ytrain, yval = train_test_split(xtrain_preprocessed, ytrain, test_size=0.0) #KAMIKAZE\n",
    "print(xtrain_preprocessed.shape)\n",
    "print(ytrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDMZ8y57il9B"
   },
   "source": [
    "In the code cell below we apply cross-validation on the XGBClassifier model. We boost the learning of many decision trees which are considered as weak leaners.\n",
    "The most important parameters are :\n",
    "- learning_rate: similar to other gradient descent methods, the classifier perform more or less a gradient correction. A small learning rate will lead to very slow convergence, a big learning rate can make us miss the sweet spot.\n",
    "- n_estimators: to be considered as a trade-off with the learning rate. It is the number of rounds performed by the classifier during learning. A too great number will lead to over-fitting.\n",
    "- max_depth: the maximum depth related to the decision trees. The less it is, the more the decision trees are basic (hence having higher bias, which is not a problem for boosting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "KY0OPTMiOC-v",
    "outputId": "874f8edc-343c-4084-81e2-5c81c925ddc8"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  boost = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,max_depth=6\n",
    "                        gamma=0, learning_rate=0.25, max_delta_step=0,\n",
    "                        min_child_weight=1, missing=None, n_estimators=700, nthread=-1,\n",
    "                        objective='binary:logistic', sampling_method='gradient_based',\n",
    "                        reg_alpha=0, reg_lambda=1, tree_method='gpu_hist',\n",
    "                        scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
    "  \n",
    "  p_grid_boost = {'max_depth': [2,3,4,5,6,7,8,9,10], 'n_estimators':[500,600,700,800,900,1000]}   \n",
    "\n",
    "  grid_boost = GridSearchCV(estimator=boost, param_grid=p_grid_boost, scoring=\"accuracy\", cv=5)\n",
    "  \n",
    "  grid_boost.fit(xtrain_preprocessed, ytrain)\n",
    "\n",
    "  print(\"Best Score: {}\".format(grid_boost.best_score_))\n",
    "  print(\"Best params: {}\".format(grid_boost.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "maMpQLxXPaI2",
    "outputId": "3817ecac-98d5-4543-ab28-bffa3123b465"
   },
   "outputs": [],
   "source": [
    "boost = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
    "                        gamma=0, learning_rate=0.25, max_delta_step=0, max_depth=6,\n",
    "                        min_child_weight=1, missing=None, n_estimators=700, nthread=-1,\n",
    "                        objective='binary:logistic', sampling_method='gradient_based',\n",
    "                        reg_alpha=0, reg_lambda=1, tree_method='gpu_hist',\n",
    "                        scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
    "boost.fit(new_x_train, new_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_-949YbQWQs"
   },
   "outputs": [],
   "source": [
    "output = boost.predict(xtest_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iIxo19DJRm50"
   },
   "outputs": [],
   "source": [
    "np.savetxt(root_dir + 'ytest_challenge_student.csv', output, fmt = '%1.0d', delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Keras-SD210-contreTous.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
