{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqc7jCJbYgQ-",
        "colab_type": "text"
      },
      "source": [
        "# SDTSIA 210 Data Challenge\n",
        "### TANG Joël\n",
        "### TORRES PÉREZ Claudia\n",
        "\n",
        "The goal of this challenge is to decide wheteher or not two faces belong to the same person, using a dataset composed of 13 features per face and 11 metrics calculated on the previous features. If the faces belong to the same person, we mark them with $1$ in the $y_{dataset}$ , otherwise its marked as $0$ in the dataset. \n",
        "\n",
        "In order to perform this binary classification task this we are going to explore different classification algorithms, and choose the one that gives us the best accuracy.\n",
        "\n",
        "###SVM\n",
        "SVM can be used with a linear kernel or not linear one. We observed that the dataset was better represented through a linear model. We performed cross-validation, the main hyper parameter was the margin tolerance. Results were good, but the model showed its limits, being unable to fit the training set.\n",
        "Another problem we had to cope with was the execution time, during the SVM construction. We used the Bagging method with SVM trained on smaller sub-datasets. This also improved the stability of the algorithm, by reducing its variance.\n",
        "\n",
        "### Random Forests\n",
        "A random forest is simply a collection of decision trees whose results are aggregated into one final result. Decision trees are a type of model used for both classification and regression, which classify the data using decision tresholds, which seems an appropriate approach for our data. Using Random Forests as a beginning model, using the gini criterion, and playing with different numbers of estimators gave us acceptable results, with accuracies of around 0.98, however, the model had a big variance, so the predicitions varied a lot from one set of training data to another, however, we liked the idea of using decision trees as the base of our model.\n",
        "\n",
        "Since decision trees are known for having a big variance, we thus had to look for a model that would help us implement it, but reduce its variance at the same time. The solution = Gradient Boosting.\n",
        "\n",
        "### XGBoost\n",
        "XGBoost is a gradient boosting algorithm. It is an iterative algorithm, based on learners which a certain base accuracy. Taken individually, these learners can have a high bias (with an accuracy slightly better than 0.5 for instance). But the algorithm will iteratively sum these learners up, by forcing each new learning to fit the error of the combination of its predecessors.\n",
        "\n",
        "We are going to use the decision trees as learners.\n",
        "\n",
        "### First, the imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEIB0o3GYm5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yWCoao9Yc2I",
        "colab_type": "text"
      },
      "source": [
        "### Loading the datasets\n",
        "We upload them from the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GpXgFgD5Y2yW",
        "outputId": "70d7bc3a-3759-4b7a-8cf8-12650854a534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load training data\n",
        "\n",
        "nrows_train = 1068504\n",
        "nrows_test = 0\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_dir = \"/content/gdrive/My Drive/SD-TSIA210_CHALLENGE/\"\n",
        "\n",
        "xtrain = np.loadtxt(root_dir + 'xtrain_challenge.csv', delimiter=',', skiprows = 1, max_rows = nrows_train + nrows_test)\n",
        "ytrain = np.loadtxt(root_dir + 'ytrain_challenge.csv', delimiter=',', skiprows = 1, max_rows = nrows_train + nrows_test)\n",
        "ytrain = np.array(ytrain).reshape(nrows_train + nrows_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rau-dQQYHbX",
        "colab_type": "text"
      },
      "source": [
        "In the cell below, we perform data augmentation techniques.\n",
        "\n",
        "Our algorithm (decision trees) doesn't require scaled data, and allows their range to be heterogeneous. However, we can still mirror the data: since each sample is composed of two faces, and a certain range of features is separable with respect to one of the two faces, we can simply exchange them. This increases the size of the training set by a 2 factor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HA2DgV5BY9Hm",
        "outputId": "0a4644cd-81b8-4b16-9777-f792a92a4cd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#%%\n",
        "# Pre-processing: we just remove the 13*2 first features, concerning only one of the two faces\n",
        "xtrain = xtrain.astype('float32')\n",
        "xtest = np.loadtxt(root_dir + 'xtest_challenge.csv', delimiter=',', skiprows = 1).astype('float32')\n",
        "\n",
        "# We change the columns\n",
        "x_train_permuter = np.copy(xtrain)\n",
        "x_train_permuter[:, :13] = xtrain[:, 13:26]\n",
        "x_train_permuter[:, 13:26] = xtrain[:, :13]\n",
        "\n",
        "new_x_train = np.concatenate((xtrain, x_train_permuter), axis=0)\n",
        "new_y_train = np.concatenate((ytrain, ytrain), axis=0)\n",
        "\n",
        "print(new_x_train.shape)\n",
        "print(new_y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EQwG-R3irRJ",
        "colab_type": "text"
      },
      "source": [
        "Now we are going to move on to the tain/test split. We are going to save 20% of our data for validating the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSW-NMOdmLjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(new_x_train, new_y_train, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtxD2BzPmZNq",
        "colab_type": "text"
      },
      "source": [
        "In the code cell below we apply cross-validation on the XGBClassifier model. We boost the learning of many decision trees which are considered as weak leaners.\n",
        "The most important parameters are :\n",
        "- learning_rate: similar to other gradient descent methods, the classifier perform more or less a gradient correction. A small learning rate will lead to very slow convergence, a big learning rate can make us miss the sweet spot.\n",
        "- n_estimators: to be considered as a trade-off with the learning rate. It is the number of rounds performed by the classifier during learning. A too great number will lead to over-fitting.\n",
        "- max_depth: the maximum depth related to the decision trees. The less it is, the more the decision trees are basic (hence having higher bias, which is not a problem for boosting)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KY0OPTMiOC-v",
        "outputId": "5977abde-9195-4bed-83bd-04860d097dc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  boost = XGBClassifierboost = XGBClassifier(colsample_bylevel=1, max_depth=6,\n",
        "                        max_delta_step=0, reg_alpha=0,\n",
        "                        min_child_weight=1, subsample=1, missing=None, nthread=-1,\n",
        "                        objective='binary:logistic', sampling_method='gradient_based',\n",
        "                         reg_lambda=1, tree_method='gpu_hist',\n",
        "                        scale_pos_weight=1.002, silent=True)\n",
        "  \n",
        "  parameters = {\"gamma\" : [0, 0.01, 0.1, 0.3],\n",
        "               \"colsample_bytree\": [0.7,0.85,1],\n",
        "               \"reg_lambda\": [1.5,2,2.5,3],\n",
        "                \"base_score\": [0.45, 0.5],\n",
        "                \"n_estimators\":[655,660,665,670,675,680,685,690,695,700],\n",
        "                \"learning_rate\":[0.15,0.2,0.25,0.35]}\n",
        "\n",
        "  xgb_rscv = RandomizedSearchCV(boost, param_distributions = parameters, scoring = \"accuracy\",\n",
        "                             cv = 5, verbose = 3, random_state = 40)\n",
        " \n",
        "  boost_fit = xgb_rscv.fit(X_train, y_train)\n",
        "\n",
        "  print(\"Best Score: {}\".format(boost_fit.best_score_))\n",
        "  print(\"Best params: {}\".format(boost_fit.best_params_))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lafiP4g6nJzZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "ab7f741c-a28d-44a6-ad7b-1aa0b9879483"
      },
      "source": [
        "y_pred= boost_fit.predict(X_test)\n",
        "print(boost.score(y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6170qC7nG5l",
        "colab_type": "text"
      },
      "source": [
        "We observed that the training accuracy was very high (> 0.999), compared with slightly less better accuracy on the validation set (0.9985). This is a sign of overfitting. We used the early stopping technique, which stops the learning phase when the validation accuracy starts to decrease."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "maMpQLxXPaI2",
        "outputId": "42c5d070-c82d-4d4b-c8b5-80d23fb58d36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "boost_2 = XGBClassifier(base_score=0.45, colsample_bylevel=1, colsample_bytree=1,max_depth=6,\n",
        "                        gamma=0, learning_rate=0.25, max_delta_step=0,\n",
        "                        min_child_weight=1, missing=None, n_estimators=167, nthread=-1,\n",
        "                        objective='binary:logistic', sampling_method='gradient_based',\n",
        "                        reg_alpha=0, reg_lambda=1, tree_method='gpu_hist',\n",
        "                        scale_pos_weight=1.005, seed=0, silent=True, subsample=1)\n",
        "\n",
        "early_stopping_rounds=15\n",
        "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
        "boost_2.fit(X_train,y_train, early_stopping_rounds=15, eval_metric=[\"error\"], eval_set=eval_set, verbose=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D-zPdwEoYdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = boost_2.predict(xtest)\n",
        "np.savetxt(root_dir + 'ytest_challenge_student.csv', y_pred, fmt = '%1.0d', delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmacZ3xmoyBt",
        "colab_type": "text"
      },
      "source": [
        "We tested the early stopping technique as an attempt to try to improve our maximal accuracy, of $0.998650512191$, yet, the results were inconclusive.\n",
        "Even after tweaking the paramaters over and over again, we found it hard to augment our accuracy by $0.00001$. We think it would be a good option to explore different data augmentation techniques, which would allow us to get a bigger training dataset, which would allow us to get better classification tresholds on our decision trees.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iIxo19DJRm50",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CHALLENGE-TANG-TORRES.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
